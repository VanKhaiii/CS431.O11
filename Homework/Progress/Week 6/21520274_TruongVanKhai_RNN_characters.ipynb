{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Data"
      ],
      "metadata": {
        "id": "qvRvv6ojR_EN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b32455-d8f4-49a1-a027-8eeb40f8e393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 6354 characters\n"
          ]
        }
      ],
      "source": [
        "text = '''… Mất em, đời này vỡ tan\n",
        "Vì câu nói này chưa, chưa từng phai, chưa từng phai oh\n",
        "Nỗi đau dày vò nỗi đau\n",
        "Mà vẫn nuốt giọt đắng, đau nhiều thêm, thêm nhiều đau ồ ố ô\n",
        "Sợ ngày thêm lâu, sợ tình thêm sâu\n",
        "Nợ lại cho nhau chiều hẹn rong chơi\n",
        "Ngọt ngào nơi tim còn nhiều không em ?\n",
        "Bầu trời thêm mây ngồi đợi mưa rơi...\n",
        "Biết đâu những cơn say đầu đời này\n",
        "Phải khóc cho đến hôm nay\n",
        "Hẹn ước chi thế em ơi giờ phải vậy\n",
        "Gửi gió nỗi nhớ bay bay...\n",
        "Nhìn hoàng hôn kia còn mang em đi xa anh thêm bao nhiêu lâu nữa sẽ trả về ?\n",
        "Đừng có như vậy, dặn lòng đừng khóc như vậy\n",
        "Mà đời đâu như anh mơ, đâu như anh mơ.\n",
        "Có thể anh sống cho em nhưng cũng có thể em sống cho em\n",
        "Quá dễ để biết tên nhau nhưng đâu có thể nhìn thấy nhau đau\n",
        "Có thể nước mắt em rơi nhưng khi quay lưng thì em sẽ cười\n",
        "Anh cũng nhắm mắt cho qua không nên cãi nữa vì anh đã lười\n",
        "Giờ em không tinh khiết, không còn cười chỉ thay vào son và phấn\n",
        "Mùa thanh xuân khi đó, em để lại cho anh biết vương và vấn\n",
        "Anh chỉ mong em hiểu, anh dành hơn nửa phần đời này che chở cho em\n",
        "Không một lời than, không một tiếng vãn anh cũng chưa từng bảo em xem, liiiii\n",
        "Sợ ngày thêm lâu, sợ tình thêm sâu\n",
        "Nợ lại cho nhau chiều hẹn rong chơi\n",
        "Ngọt ngào nơi tim còn nhiều không em\n",
        "Bầu trời thêm mây ngồi đợi mưa rơi\n",
        "Biết đâu những cơn say đầu đời này\n",
        "Phải khóc cho đến hôm nay\n",
        "Hẹn ước chi thế em ơi giờ phải vậy\n",
        "Gửi gió nỗi nhớ bay bay\n",
        "Nhìn hoàng hôn kia còn mang em đi xa anh thêm bao nhiêu lâu nữa sẽ trả về\n",
        "Đừng có như vậy, dặn lòng đừng khóc như vậy\n",
        "Mà đời đâu như anh mơ...\n",
        "… Em mơ một mai thức giấc chẳng cơn đau nào sẽ ghé qua\n",
        "Em mong ngày mai có nắng sưởi ấm con tim buồn đau tối qua\n",
        "Em mong dù cho vấp ngã có lắm phong ba bình yên chở che\n",
        "Em đau trời cao có thấu, có biết cơn đau mùi hương thế nào.\n",
        "… Anh mơ một mai thức giấc chẳng cơn đau nào sẽ ghé qua\n",
        "Anh mong ngày mai có nắng sưởi ấm con tim buồn đau tối qua\n",
        "Anh mong dù cho vấp ngã có lắm phong ba bình yên chở che\n",
        "Anh đau trời cao có thấu, có biết cơn đau mùi hương thế nào.\n",
        "… Biết đâu những cơn say đầu đời này\n",
        "Phải khóc cho đến hôm nay\n",
        "Hẹn ước chi thế em ơi giờ phải vậy\n",
        "Gửi gió nỗi nhớ bay bay\n",
        "Nhìn hoàng hôn kia còn mang em đi xa anh thêm bao nhiêu lâu nữa sẽ trả về\n",
        "Đừng có như vậy, dặn lòng đừng khóc như vậy\n",
        "Mà đời đâu như anh mơ.\n",
        "\n",
        "\n",
        "Anh sẽ chờ em\n",
        "Dù biển xanh kia có cạn khô\n",
        "Dù qua thêm bao kiếp\n",
        "Anh vẫn sẽ chờ\n",
        "Nhân thế khổ đau\n",
        "Tìm hoài sao không thấy nhau\n",
        "Người thương chẳng thương mình\n",
        "Còn người không thương cứ theo ta cả một đời\n",
        "Cây đã già nua\n",
        "Chờ ngày chết cách xa cõi đời\n",
        "Chiều hoàng hôn buông xuống phía tây nghẹn ngào\n",
        "Uống chén tình say\n",
        "Hoạ người thương trong bức tranh\n",
        "Hoạ ánh mắt anh buồn\n",
        "Hoạ nụ cười thêm trên nét môi\n",
        "Nhưng sao không được\n",
        "Nhiều lần em muốn anh vui mà thôi\n",
        "Hoạ vào nét môi anh nở cười tươi\n",
        "Dòng lệ rớt rơi phai màu\n",
        "Nhoè đi nụ cười khi ấy\n",
        "Lòng thì đau đớn trong em khổ đau\n",
        "Mà chẳng dám khóc đâu ai hiểu thấu\n",
        "Vẫn hoạ thêm chiếc môi cười tiếp theo\n",
        "Nhưng đau thấu trời\n",
        "Cây đã già nua\n",
        "Chờ ngày chết cách xa cõi đời\n",
        "Chiều hoàng hôn buông xuống phía tây nghẹn ngào\n",
        "Lỡ chén tình say\n",
        "Hoạ người thương trong bức tranh\n",
        "Hoạ ánh mắt anh buồn\n",
        "Tìm nụ cười anh trên nét môi\n",
        "Nhưng sao chẳng thấy\n",
        "Nhiều lần em muốn anh vui mà thôi\n",
        "Hoạ vào nét môi anh nở cười tươi\n",
        "Dòng lệ rớt rơi phai màu\n",
        "Nhoè đi nụ cười khi ấy\n",
        "Lòng thì đau đớn trong em khổ đau\n",
        "Mà chẳng dám khóc đâu ai hiểu thấu\n",
        "Vẫn hoạ thêm chiếc môi cười tiếp theo\n",
        "Nhưng đau thấu trời\n",
        "Khóc thật nhiều\n",
        "Ngồi khóc thật nhiều\n",
        "Khóc cho đời phong ba lắm đau mà chẳng nói ra\n",
        "Đến khi già\n",
        "Buồn hết một đời\n",
        "Đớn đau này của anh sẽ cao hơn trời\n",
        "Nhiều lần em muốn anh vui mà thôi\n",
        "Hoạ vào nét môi em nở cười tươi\n",
        "Dòng lệ rớt rơi phai màu\n",
        "Nhoè đi nụ cười khi ấy\n",
        "Lòng thì đau đớn trong em khổ đau\n",
        "Mà chẳng dám khóc đâu ai hiểu thấu\n",
        "Vẫn hoạ thêm chiếc môi cười tiếp theo\n",
        "Nhưng đau ai thấu\n",
        "Vẫn hoạ thêm chiếc môi cười lần nữa\n",
        "Nhưng đau ai thấu?\n",
        "\n",
        "\n",
        "Ai bánh mì không?\n",
        "Ai bánh mì không?\n",
        "Bánh mì không, cô ơi?\n",
        "Cô ăn xôi rồi, con ơi\n",
        "Khi mà không còn yêu thì, đêm nào ta cũng say vì\n",
        "Không còn ai gọi ta về mau thật mau, đau thật đau\n",
        "Căn nhà ta cùng thuê ở, giờ đây đã có người thuê rồi\n",
        "Người ta nhìn thật hạnh phúc, bọn mình bơ vơ, thời mình ngu ngơ\n",
        "Em thì quá bận với những khoản lo\n",
        "Nhiều hôm ở công ty trễ lương thì tối ăn bánh mì\n",
        "(Bánh mì không, không có thịt luôn)\n",
        "Tay cầm đơn cầu mong anh có công việc mới phụ em lúc khó khăn\n",
        "Những ngày tháng rất căng (haizz)\n",
        "Nhớ xưa còn nói nếu lúc mình giàu\n",
        "Hai ta có nhà lầu, và thêm xe hơi có đủ tất cả màu\n",
        "Chiếc xe cùi bắp bể bánh ngoài đường (chiếc xe cùi bắp bể bánh)\n",
        "Hai ta thấy bình thường (ta thấy bình thường)\n",
        "Dù mồ hôi tuôn rơi nhưng có nhau đời thêm vui\n",
        "Nhớ xưa mình thích những món lề đường\n",
        "Hủ tiếu gõ và mì gói nước sôi, thêm cái trứng gà\n",
        "Đến nay điều ước cũng đã thành thật\n",
        "Thành phố mỗi lúc càng chật, ta mất nhau, dễ gì gặp lại nhau\n",
        "Rối ren như trật tự giao đường phố (oh)\n",
        "Lạc nhau rồi (ha-hah), thôi đừng cố\n",
        "Đèn xanh, đèn đỏ (haizz) rồi tới vàng\n",
        "Dừng hay là chạy thì chuyện buồn ta vẫn mang\n",
        "Tiếng rao thân thuộc của những quán quen xưa\n",
        "Mỗi lần nghe thế lòng như cắt, như cưa\n",
        "Nhiều tiền anh biết bọn mình được ấm no\n",
        "Mà tiền nhiều như thế nên tình cảm mình hoá tro\n",
        "Ta sợ mình đói nên lao vào kiếm ăn\n",
        "Rồi trái tim ta lạnh, lạnh hơn một phiến băng (đúng vậy)\n",
        "Ta vẫn chưa rõ tại sao mình thay đổi\n",
        "Không ăn cùng nhau, nhiều công việc nên hay vội (anh cũng không biết nữa)\n",
        "Có những bản nhạc không nên hát cùng nhau\n",
        "Vì khi bài nhạc cất lên thì cả hai người cùng đau (đau)\n",
        "Lúc nghèo rất vui, những năm tháng thăng trầm ấy\n",
        "Chứ không phải giống bây giờ, anh mặc vest, còn em diện váy đâu\n",
        "Em thì quá bận với những khoản lo\n",
        "Nhiều hôm ở công ty trễ lương thì tối ăn bánh mì\n",
        "(Bánh mì không, không có thịt luôn)\n",
        "Tay cầm đơn cầu mong anh có công việc mới phụ em lúc khó khăn\n",
        "Những ngày tháng rất căng (căng, căng, căng)\n",
        "Nhớ xưa còn nói nếu lúc mình giàu\n",
        "Hai ta có nhà lầu, và lên xe hơi có đủ tất cả màu\n",
        "Chiếc xe cùi bắp bể bánh ngoài đường (chiếc xe cùi bắp bể bánh)\n",
        "Hai ta thấy bình thường (ta thấy bình thường)\n",
        "Dù mồ hôi tuôn rơi nhưng có nhau đời thêm vui\n",
        "Nhớ xưa mình thích những món lề đường\n",
        "Hủ tiếu gõ và mì gói nước sôi, thêm cái trứng gà (no, no, no, no, no, no)\n",
        "Đến nay điều ước cũng đã thành thật (oh-oh)\n",
        "Thành phố mỗi lúc càng trật (oh-oh)\n",
        "Ta mất nhau, câu chuyện cười thật đau (oh-oh-oh)\n",
        "Em thì quá bận với những khoản lo\n",
        "Nhiều hôm ở công ty trễ lương thì tối ăn bánh mì\n",
        "Tay cầm đơn cầu mong anh có công việc mới phụ em lúc khó khăn\n",
        "Những ngày tháng rất căng (haizz)\n",
        "'''\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a12c60a-8f76-4e7c-a32b-89d22002396f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "… Mất em, đời này vỡ tan\n",
            "Vì câu nói này chưa, chưa từng phai, chưa từng phai oh\n",
            "Nỗi đau dày vò nỗi đau\n",
            "Mà vẫn nuốt giọt đắng, đau nhiều thêm, thêm nhiều đau ồ ố ô\n",
            "Sợ ngày thêm lâu, sợ tình thêm sâu\n",
            "Nợ lại cho nhau chiều hẹn rong chơi\n",
            "Ngọt ngào nơi ti\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e68b7b5-fd47-4580-dd38-ec4bce5a857e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 6354 characters\n",
            "… Mất em, đời này vỡ tan Vì câu nói này chưa, chưa từng phai, chưa từng phai oh Nỗi đau dày vò nỗi đau Mà vẫn nuốt giọt đắng, đau nhiều thêm, thêm nhiều đau ồ ố ô Sợ ngày thêm lâu, sợ tình thêm sâu Nợ lại cho nhau chiều hẹn rong chơi Ngọt ngào nơi ti\n"
          ]
        }
      ],
      "source": [
        "def prepocessing_data(text):\n",
        "  text = re.sub(r'[\\t\\n]', ' ', text)\n",
        "  return text\n",
        "\n",
        "text = prepocessing_data(text)\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iQl8nKp0cWk",
        "outputId": "1b743a8e-a0ca-425b-c1bb-5c7abf283cd4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107 unique characters\n",
            "[' ', '(', ')', ',', '-', '.', '?', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'à', 'á', 'â', 'ã', 'è', 'é', 'ê', 'ì', 'í', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ă', 'Đ', 'đ', 'ũ', 'ơ', 'ư', 'ạ', 'ả', 'ấ', 'ầ', 'ẫ', 'ậ', 'ắ', 'ẳ', 'ặ', 'ẹ', 'ẽ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'ụ', 'ủ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=vocab,\n",
        "    mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "-bRB7zq40h1X"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(ids)"
      ],
      "metadata": {
        "id": "HGNKsZlm1GX4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74788b6-1dab-4cbb-8d01-d6b33e7053bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "… Mất em, đời này vỡ tan Vì câu nói này chưa, chưa từng phai, chưa từng phai oh Nỗi đau dày vò nỗi đa\n"
          ]
        }
      ],
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for seq in sequences.take(1):\n",
        "  print(text_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee78d460-f687-4a19-a0c8-aef19557a94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : … Mất em, đời này vỡ tan Vì câu nói này chưa, chưa từng phai, chưa từng phai oh Nỗi đau dày vò nỗi đ\n",
            "Target:  Mất em, đời này vỡ tan Vì câu nói này chưa, chưa từng phai, chưa từng phai oh Nỗi đau dày vò nỗi đa\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example))\n",
        "    print(\"Target:\", text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc0763e-ce23-44b5-9364-7619ed33009d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(2, 100), dtype=tf.int64, name=None), TensorSpec(shape=(2, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce017772-dda3-4fd7-8f5d-7cc1a2f9048d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 100, 108) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da909eec-9bde-4736-e5cb-657212f1c2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  27648     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  110700    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4076652 (15.55 MB)\n",
            "Trainable params: 4076652 (15.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7e44c5-7e45-4f62-be61-cd9266f6d084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (2, 100, 108)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.6806397, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c1c14d-47b6-4d41-f105-1abf77e85fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "31/31 [==============================] - 5s 87ms/step - loss: 4.1935\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 1s 28ms/step - loss: 3.0217\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 2.5315\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 1s 28ms/step - loss: 2.2583\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 2.0610\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 1.9204\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 1.7866\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 1.6331\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 1s 28ms/step - loss: 1.5031\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 1.3044\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 1s 29ms/step - loss: 1.0996\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.8825\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.6584\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.4557\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3115\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.2265\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.1622\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 1s 29ms/step - loss: 0.1254\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0928\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0873\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0721\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0653\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0604\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0537\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0532\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0444\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0424\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0423\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0416\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0375\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.0391\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 0.0353\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0348\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0392\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0361\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0356\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0329\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 2s 54ms/step - loss: 0.0320\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0310\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0309\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0298\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.0287\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0307\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0303\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0311\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.0298\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 0.0286\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 1s 45ms/step - loss: 0.0286\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 0.0286\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 0.0328\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    print(predicted_logits)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4f5e8c-895b-462a-e075-57f10a3797f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". D em xem mì côi đưng cho em Quá dễ để biết tên nhau nhưng đâu có thể nhìn thấy nhau đau Chiều hoàng hôn buông xuống phía tây nghẹn ngào Uống phai oh Nỗi đau dày vò nỗi đau trời cao có thấu, có biết cơn đau mùi hương thế nào. … Biết đâu những cơn say đầu đời này Phải khóc cho đến hôm nay Hẹn ước chi thế em ơi giờ phải vậy Gửi gió nỗi nhớ bay bay Nhìn hoàng hôn kia còn mang em đi xa anh thêm bao nhiêu lâu nữa sẽ trả về Đừng có như vậy, dặn lòng đừng khóc như vậy Mà đời đâu như anh mơ, đâu như anh \n",
            "\n",
            "________________________________________________________________________________\n",
            "Lenght text generation:  501\n",
            "\n",
            "Run time: 2.8114852905273438\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "print(\"Lenght text generation: \",len(result[0].numpy().decode('utf-8')))\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ae3Vuly-P1v0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}